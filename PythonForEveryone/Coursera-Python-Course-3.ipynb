{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 - Chapter 11.1 - Regular Expressions\n",
    "## Understanding Regular Expressions\n",
    "\n",
    "Refer to [Regular Expression Quick Guide](https://www.py4e.com/html3/11-regex)\n",
    "\n",
    "`^` Matches the beginning of the line.\n",
    "\n",
    "`$` Matches the end of the line.\n",
    "\n",
    "`.` Matches any character (a wildcard).\n",
    "\n",
    "`\\s` Matches a whitespace character.\n",
    "\n",
    "`\\S` Matches a non-whitespace character (opposite of \\s).\n",
    "\n",
    "`*` Applies to the immediately preceding character(s) and indicates to match zero or more times.\n",
    "\n",
    "`*?` Applies to the immediately preceding character(s) and indicates to match zero or more times in “non-greedy mode”.\n",
    "\n",
    "`+` Applies to the immediately preceding character(s) and indicates to match one or more times.\n",
    "\n",
    "`+?` Applies to the immediately preceding character(s) and indicates to match one or more times in “non-greedy mode”.\n",
    "\n",
    "`?` Applies to the immediately preceding character(s) and indicates to match zero or one time.\n",
    "\n",
    "`??` Applies to the immediately preceding character(s) and indicates to match zero or one time in “non-greedy mode”.\n",
    "\n",
    "`[aeiou]` Matches a single character as long as that character is in the specified set. In this example, it would match “a”, “e”, “i”, “o”, or “u”, but no other characters.\n",
    "\n",
    "`[a-z0-9]` You can specify ranges of characters using the minus sign. This example is a single character that must be a lowercase letter or a digit.\n",
    "\n",
    "`[^A-Za-z]` When the first character in the set notation is a caret, it inverts the logic. This example matches a single character that is anything other than an uppercase or lowercase letter.\n",
    "\n",
    "`( )` When parentheses are added to a regular expression, they are ignored for the purpose of matching, but allow you to extract a particular subset of the matched string rather than the whole string when using findall().\n",
    "\n",
    "`\\b` Matches the empty string, but only at the start or end of a word.\n",
    "\n",
    "`\\B` Matches the empty string, but not at the start or end of a word.\n",
    "\n",
    "`\\d` Matches any decimal digit; equivalent to the set [0-9].\n",
    "\n",
    "`\\D` Matches any non-digit character; equivalent to the set [^0-9]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `re.search()` like `find()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: stephen.marquard@uct.ac.za\n",
      "From: louis@media.berkeley.edu\n",
      "From: zqian@umich.edu\n",
      "From: rjlowe@iupui.edu\n",
      "From: zqian@umich.edu\n",
      "From: rjlowe@iupui.edu\n",
      "From: cwen@iupui.edu\n",
      "From: cwen@iupui.edu\n",
      "From: gsilver@umich.edu\n",
      "From: gsilver@umich.edu\n",
      "From: zqian@umich.edu\n",
      "From: gsilver@umich.edu\n",
      "From: wagnermr@iupui.edu\n",
      "From: zqian@umich.edu\n",
      "From: antranig@caret.cam.ac.uk\n",
      "From: gopal.ramasammycook@gmail.com\n",
      "From: david.horwitz@uct.ac.za\n",
      "From: david.horwitz@uct.ac.za\n",
      "From: david.horwitz@uct.ac.za\n",
      "From: david.horwitz@uct.ac.za\n",
      "From: stephen.marquard@uct.ac.za\n",
      "From: louis@media.berkeley.edu\n",
      "From: louis@media.berkeley.edu\n",
      "From: ray@media.berkeley.edu\n",
      "From: cwen@iupui.edu\n",
      "From: cwen@iupui.edu\n",
      "From: cwen@iupui.edu\n"
     ]
    }
   ],
   "source": [
    "hand = open('mbox-short.txt')\n",
    "for line in hand:\n",
    "    line = line.rstrip()\n",
    "    if line.find('From:') >= 0:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: stephen.marquard@uct.ac.za\n",
      "From: louis@media.berkeley.edu\n",
      "From: zqian@umich.edu\n",
      "From: rjlowe@iupui.edu\n",
      "From: zqian@umich.edu\n",
      "From: rjlowe@iupui.edu\n",
      "From: cwen@iupui.edu\n",
      "From: cwen@iupui.edu\n",
      "From: gsilver@umich.edu\n",
      "From: gsilver@umich.edu\n",
      "From: zqian@umich.edu\n",
      "From: gsilver@umich.edu\n",
      "From: wagnermr@iupui.edu\n",
      "From: zqian@umich.edu\n",
      "From: antranig@caret.cam.ac.uk\n",
      "From: gopal.ramasammycook@gmail.com\n",
      "From: david.horwitz@uct.ac.za\n",
      "From: david.horwitz@uct.ac.za\n",
      "From: david.horwitz@uct.ac.za\n",
      "From: david.horwitz@uct.ac.za\n",
      "From: stephen.marquard@uct.ac.za\n",
      "From: louis@media.berkeley.edu\n",
      "From: louis@media.berkeley.edu\n",
      "From: ray@media.berkeley.edu\n",
      "From: cwen@iupui.edu\n",
      "From: cwen@iupui.edu\n",
      "From: cwen@iupui.edu\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "hand = open('mbox-short.txt')\n",
    "for line in hand:\n",
    "    line = line.rstrip()\n",
    "    if re.search('From:', line):\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wild-Card Characters\n",
    "- The dot character matches any character\n",
    "- If you add the asterisk character, the character is \"any number of times\"\n",
    "\n",
    "Use `^X.*:`\n",
    "\n",
    "For this example:\n",
    "\n",
    "```\n",
    "X-Sieve: CMU Sieve 2.3\n",
    "X-DSPAM-Result: Innocent\n",
    "X-DSPAM-Confidence: 0.8475\n",
    "X-Content-Type-Message-Body: text/plain\n",
    "```\n",
    "\n",
    "## Fine-Tuning Your Match\n",
    "\n",
    "Use `^X-\\S+:`\n",
    "\n",
    "For this example:\n",
    "\n",
    "```\n",
    "X-Sieve: CMU Sieve 2.3\n",
    "X-DSPAM-Result: Innocent\n",
    "X-Plane is behind schedule: two weeks\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching and Extracting Data\n",
    "- `re.search()` returns a T/F depending on whether the string matches the regular expression\n",
    "- If we want to extract the matching strings, use `re.findall()`\n",
    "- `[0-9]+` means one or more digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '6', '17']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# returns a list of matching numbers\n",
    "x = 'My 2 favourite numbers are 6 and 17'\n",
    "y = re.findall('[0-9]+',x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# there are no upper case vowels in the string x, returns empty list\n",
    "y = re.findall('[AEIOU]+', x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning: Greedy Matching\n",
    "- The repeat charracters `(*` and `+)` push outward in both directions (greedy) to match the largest possible string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From: Using the :']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "x = 'From: Using the : character'\n",
    "y = re.findall('^F.+:', x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Greedy Matching\n",
    "- Not all regular expression repeat codes are greedy!\n",
    "- If you add a `?` character, the `+` and `*` chill out a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From:']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "x = 'From: Using the : character'\n",
    "y = re.findall('^F.+?:', x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning String Extraction\n",
    "- You can redefine the match for `re.findall()` and separately determine which portion of the match is to be extracted by using parenthesis.\n",
    "- `\\S+` means at least one non-whitespace character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stephen.marquard@uct.ac.za']\n"
     ]
    }
   ],
   "source": [
    "x = 'From stephen.marquard@uct.ac.za Sat Jun  5 09:14:16 2008'\n",
    "y = re.findall('\\S+@\\S+', x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stephen.marquard@uct.ac.za']\n"
     ]
    }
   ],
   "source": [
    "# Match the 'From email@email' but extract just the email portion\n",
    "x = 'From stephen.marquard@uct.ac.za Sat Jun  5 09:14:16 2008'\n",
    "y = re.findall('From (\\S+@\\S+)', x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Double Split Pattern (Old)\n",
    "- Sometimes we split a line one way, then grab one of t he pieces of the line and split that piece again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From', 'stephen.marquard@uct.ac.za', 'Sat', 'Jun', '5', '09:14:16', '2008']\n",
      "stephen.marquard@uct.ac.za\n",
      "uct.ac.za\n"
     ]
    }
   ],
   "source": [
    "line = 'From stephen.marquard@uct.ac.za Sat Jun  5 09:14:16 2008'\n",
    "\n",
    "words = line.split()\n",
    "print(words)\n",
    "\n",
    "email = words[1]\n",
    "print(email)\n",
    "\n",
    "pieces = email.split('@')\n",
    "print(pieces[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Double Split Pattern (Regex Version)\n",
    "`@([^ ]*)` look thru the string until you find an at sign\n",
    "\n",
    "`[^ ]` match non-blank character\n",
    "\n",
    "`*` match many of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uct.ac.za']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "line = 'From stephen.marquard@uct.ac.za Sat Jun  5 09:14:16 2008'\n",
    "y = re.findall('@([^ ]*)', line)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Double Split Pattern (Even Cooler Regex Version)\n",
    "`^From .*@([^ ]*)` \n",
    "\n",
    "Start at the beginning of the line, look for the string 'From\n",
    "\n",
    "look thru the string until you find an at sign\n",
    "\n",
    "`[^ ]` match non-blank character\n",
    "\n",
    "`*` match many of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uct.ac.za']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "line = 'From stephen.marquard@uct.ac.za Sat Jun  5 09:14:16 2008'\n",
    "y = re.findall('^From .*@([^ ]*)', line)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spam Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum: 0.9907\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "hand = open('mbox-short.txt')\n",
    "numlist = list()\n",
    "for line in hand:\n",
    "    line = line.rstrip()\n",
    "    stuff = re.findall('X-DSPAM-Confidence: ([0-9.]+)', line)\n",
    "    # stuff returns a list of numbers\n",
    "    # if there are more than 1 item in list, means something wrong\n",
    "    if len(stuff) != 1 : continue\n",
    "    num = float(stuff[0])\n",
    "    numlist.append(num)\n",
    "print('Maximum:', max(numlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escape Character\n",
    "- If you want a special regular expression character to just behave normally (most of the time) you prefix it with a '\\'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$10.00']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "x = 'We just received $10.00 for cookies.'\n",
    "y = re.findall('\\$[0-9.]+', x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uct.ac.za']\n"
     ]
    }
   ],
   "source": [
    "# quiz\n",
    "import re\n",
    "line = 'From stephen.marquard@uct.ac.za Sat Jun  5 09:14:16 2008'\n",
    "y = re.findall('@(\\S+)', line)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From: Using the :']\n"
     ]
    }
   ],
   "source": [
    "x = 'From: Using the : character'\n",
    "y = re.findall('^F.+:', x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stephen.marquard@uct.ac.za']\n"
     ]
    }
   ],
   "source": [
    "# quiz\n",
    "import re\n",
    "line = 'From stephen.marquard@uct.ac.za Sat Jun  5 09:14:16 2008'\n",
    "y = re.findall('\\S+?@\\S+', line)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Extracting Data With Regular Expressions\n",
    "\n",
    "Finding Numbers in a Haystack\n",
    "\n",
    "In this assignment you will read through and parse a file with text and numbers. You will extract all the numbers in the file and compute the sum of the numbers.\n",
    "\n",
    "Data Files\n",
    "We provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment.\n",
    "\n",
    "Sample data: http://py4e-data.dr-chuck.net/regex_sum_42.txt (There are 90 values with a sum=445833)\n",
    "\n",
    "Actual data: http://py4e-data.dr-chuck.net/regex_sum_1088547.txt (There are 67 values and the sum ends with 236)\n",
    "\n",
    "These links open in a new window. Make sure to save the file into the same folder as you will be writing your Python program. Note: Each student will have a distinct data file for the assignment - so only use your own data file for analysis.\n",
    "\n",
    "Data Format\n",
    "\n",
    "The file contains much of the text from the introduction of the textbook except that random numbers are inserted throughout the text. Here is a sample of the output you might see:\n",
    "\n",
    "```\n",
    "Why should you learn to write programs? 7746\n",
    "12 1929 8827\n",
    "Writing programs (or programming) is a very creative \n",
    "7 and rewarding activity.  You can write programs for \n",
    "many reasons, ranging from making your living to solving\n",
    "8837 a difficult data analysis problem to having fun to helping 128\n",
    "someone else solve a problem.  This book assumes that \n",
    "everyone needs to know how to program ...\n",
    "```\n",
    "The sum for the sample text above is 27486. The numbers can appear anywhere in the line. There can be any number of numbers in each line (including none).\n",
    "\n",
    "**Handling The Data**\n",
    "\n",
    "The basic outline of this problem is to read the file, look for integers using the re.findall(), looking for a regular expression of '[0-9]+' and then converting the extracted strings to integers and summing up the integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum: 318236\n"
     ]
    }
   ],
   "source": [
    "# import regex\n",
    "import re\n",
    "sum = 0\n",
    "\n",
    "# open the file, use try-except to handle exceptions gracefully\n",
    "try:\n",
    "    fh = open('regex_sum_1088547.txt')\n",
    "except:\n",
    "    print(\"Unable to open file!\")\n",
    "    quit()\n",
    "\n",
    "# read the file and use regular expressions to extract the numbers\n",
    "for line in fh:\n",
    "    line = line.rstrip()\n",
    "    extracted = re.findall('[0-9]+', line)\n",
    "     # if the extracted list of strings has length 0, means no numbers detected. Thus, skip (continue)\n",
    "    if len(extracted) == 0 : continue\n",
    "    #print(extracted)\n",
    "    \n",
    "    # for each number in the list of strings, parse to integers and sum them up\n",
    "    for number in extracted:\n",
    "        sum = sum + int(number)\n",
    "        \n",
    "print(\"Sum:\", sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Just for Fun\n",
    "There are a number of different ways to approach this problem. While we don't recommend trying to write the most compact code possible, it can sometimes be a fun exercise. Here is a a redacted version of two-line version of this program using list comprehension:\n",
    "\n",
    "```\n",
    "import re\n",
    "print( sum( [ ****** *** * in **********('[0-9]+',**************************.read()) ] ) )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 3 - Chapter 12 - Networked Technology\n",
    "## Hypertext Transfer Protocol (HTTP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\n",
      "Date: Wed, 02 Dec 2020 15:42:36 GMT\n",
      "Server: Apache/2.4.18 (Ubuntu)\n",
      "Last-Modified: Sat, 13 May 2017 11:22:22 GMT\n",
      "ETag: \"a7-54f6609245537\"\n",
      "Accept-Ranges: bytes\n",
      "Content-Length: 167\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\n",
      "Pragma: no-cache\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\n",
      "Connection: close\n",
      "Content-Type: text/plain\n",
      "\n",
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already s\n",
      "ick and pale with grief\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "# init new socket\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "# connect to stated URL at port 80\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "\n",
    "# encode the string to send aka convert Unicode to UTF-8\n",
    "# because strings in Python are in Unicode, but you need to send in UTF-8 bytes.\n",
    "cmd = 'GET http://data.pr4e.org/romeo.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "\n",
    "# send encoded string to server\n",
    "mysock.send(cmd)\n",
    "\n",
    "# keep looping, keep receiving & printing data until we hit end of transmission & break out of loop\n",
    "while True: \n",
    "    data = mysock.recv(512) # use receive method of socket to get data, receive up to 512 chars\n",
    "    # if we get no data, that means end of transmission\n",
    "    if (len(data) < 1):\n",
    "        break\n",
    "    print(data.decode())\n",
    "\n",
    "# close the socket once done\n",
    "mysock.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment - Understanding the Request / Response Cycle\n",
    "\n",
    "Exploring the HyperText Transport Protocol\n",
    "\n",
    "You are to retrieve the following document using the HTTP protocol in a way that you can examine the HTTP Response headers.\n",
    "\n",
    "http://data.pr4e.org/intro-short.txt\n",
    "\n",
    "There are three ways that you might retrieve this web page and look at the response headers:\n",
    "\n",
    "Preferred: Modify the socket1.py program to retrieve the above URL and print out the headers and data.\n",
    "\n",
    "Make sure to change the code to retrieve the above URL - the values are different for each URL.\n",
    "\n",
    "Open the URL in a web browser with a developer console or FireBug and manually examine the headers that are returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\n",
      "Date: Wed, 02 Dec 2020 16:18:45 GMT\n",
      "Server: Apache/2.4.18 (Ubuntu)\n",
      "Last-Modified: Sat, 13 May 2017 11:22:22 GMT\n",
      "ETag: \"1d3-54f6609240717\"\n",
      "Accept-Ranges: bytes\n",
      "Content-Length: 467\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\n",
      "Pragma: no-cache\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\n",
      "Connection: close\n",
      "Content-Type: text/plain\n",
      "\n",
      "Why should you learn to write programs?\n",
      "\n",
      "Writing programs (or programming) is a very creative \n",
      "and rewarding activity.  You can write programs\n",
      " for \n",
      "many reasons, ranging from making your living to solving\n",
      "a difficult data analysis problem to having fun to helping\n",
      "someone else solve a problem.  This book assumes that \n",
      "everyone needs to know how to program, and that once \n",
      "you know how to program you will figure out what you want \n",
      "to do with your newfound skills.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "# init new socket\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "# connect to stated URL at port 80\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "\n",
    "# encode the string to send aka convert Unicode to UTF-8\n",
    "# because strings in Python are in Unicode, but you need to send in UTF-8 bytes.\n",
    "cmd = 'GET http://data.pr4e.org/intro-short.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "\n",
    "# send encoded string to server\n",
    "mysock.send(cmd)\n",
    "\n",
    "# keep looping, keep receiving & printing data until we hit end of transmission & break out of loop\n",
    "while True: \n",
    "    data = mysock.recv(512) # use receive method of socket to get data, receive up to 512 chars\n",
    "    # if we get no data, that means end of transmission\n",
    "    if (len(data) < 1):\n",
    "        break\n",
    "    print(data.decode())\n",
    "\n",
    "# close the socket once done\n",
    "mysock.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 - Chapter 12 - Unicode Characters and Strings\n",
    "\n",
    "- ASCII (Mapping for numbers to characters)\n",
    "\n",
    "### Representing Simple Strings\n",
    "\n",
    "- Each character is represented by a number between 0 and 256 stored in 8 bits of memory.\n",
    "\n",
    "- We refer to \"8 bits of memory as a byte\" of memory (i.e. my disk drive contains 3 Terabytes of memory)\n",
    "\n",
    "- The `ord()` function tells us the numeric value of a simple ASCII character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "101\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# uppercase has lower ordinal than lowercase\n",
    "print(ord('H'))\n",
    "print(ord('e'))\n",
    "print(ord('\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Byte Characters\n",
    "\n",
    "- To represent the wide range of characters computers must handle, we represent characters with more than one byte\n",
    "\n",
    "    - UTF-16 - Fixed length - Two bytes\n",
    "    \n",
    "    - UTF-32 - Fixed length - Four bytes\n",
    "    \n",
    "    - UTF-8 - 1~4 bytes\n",
    "    \n",
    "      - Upwards compatible with ASCII\n",
    "      \n",
    "      - Automatic detection between ASCII and UTF-8\n",
    "      \n",
    "      - UTF-8 is recommended practice for encoding data to be exchanged between systems\n",
    "      \n",
    "- In Python, all strings are unicode.\n",
    "\n",
    "- When we talk to a network resource using sockets or talk to a database we have to encode and decode data (usually to UTF-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Strings to Bytes\n",
    "\n",
    "- When we talk to an external resource like a network socket,  we send bytes.\n",
    "\n",
    "- So, we need to encode Python 3 strings into a given character encoding.\n",
    "\n",
    "- When we read data from an external source, we must decode it based on the character set so it is properly represented in Python 3 as a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Web Pages using `urllib` in Python\n",
    "\n",
    "- We have a library that does all the socket work for us and make web pages look like a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "fhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "for line in fhand:\n",
    "    print(line.decode().strip())\n",
    "    \n",
    "counts = dict()\n",
    "for line in fhand:\n",
    "    words = line.decode().split()\n",
    "    for word in words:\n",
    "        counts[word] = counts.get(word, 0) + 1\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading web pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>The First Page</h1>\n",
      "<p>\n",
      "If you like, you can switch to the\n",
      "<a href=\"http://www.dr-chuck.com/page2.htm\">\n",
      "Second Page</a>.\n",
      "</p>\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "fhand = urllib.request.urlopen('http://www.dr-chuck.com/page1.htm')\n",
    "for line in fhand:\n",
    "    print(line.decode().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping\n",
    "\n",
    "- When a program or script pretends to be a browser and retrieves web pages, looks at those web pages, extracts information and then looks at more web pages\n",
    "\n",
    "- Search engines scrape web pages - we call this \"spidering the web\" or \"web crawling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another library: Beautiful Soup\n",
    "\n",
    "- Library to easily parse web pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sherna\\anaconda3\\lib\\site-packages (4.9.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sherna\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter - https://pypi.org/project/beautifulsoup4/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#content\n",
      "https://www.python.org/psf/donations/2020-q42020-drive/\n",
      "/\n",
      "/help/\n",
      "/sponsor/\n",
      "/account/login/\n",
      "/account/register/\n",
      "/help/\n",
      "/sponsor/\n",
      "/account/login/\n",
      "/account/register/\n",
      "/project/beautifulsoup4/\n",
      "#description\n",
      "#history\n",
      "#files\n",
      "http://www.crummy.com/software/BeautifulSoup/bs4/\n",
      "http://www.crummy.com/software/BeautifulSoup/bs4/download/\n",
      "https://libraries.io/pypi/beautifulsoup4\n",
      "https://packaging.python.org/guides/analyzing-pypi-package-downloads/\n",
      "mailto:leonardr@segfault.org\n",
      "/user/leonard/\n",
      "/search/?c=Development+Status+%3A%3A+5+-+Production%2FStable\n",
      "/search/?c=Intended+Audience+%3A%3A+Developers\n",
      "/search/?c=License+%3A%3A+OSI+Approved+%3A%3A+MIT+License\n",
      "/search/?c=Programming+Language+%3A%3A+Python\n",
      "/search/?c=Programming+Language+%3A%3A+Python+%3A%3A+2.7\n",
      "/search/?c=Programming+Language+%3A%3A+Python+%3A%3A+3\n",
      "/search/?c=Topic+%3A%3A+Software+Development+%3A%3A+Libraries+%3A%3A+Python+Modules\n",
      "/search/?c=Topic+%3A%3A+Text+Processing+%3A%3A+Markup+%3A%3A+HTML\n",
      "/search/?c=Topic+%3A%3A+Text+Processing+%3A%3A+Markup+%3A%3A+SGML\n",
      "/search/?c=Topic+%3A%3A+Text+Processing+%3A%3A+Markup+%3A%3A+XML\n",
      "#description\n",
      "#data\n",
      "#history\n",
      "#files\n",
      "http://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
      "http://www.crummy.com/software/BeautifulSoup/bs4/\n",
      "http://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
      "http://groups.google.com/group/beautifulsoup/\n",
      "https://code.launchpad.net/beautifulsoup/\n",
      "https://bugs.launchpad.net/beautifulsoup/\n",
      "https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG\n",
      "https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=readme\n",
      "https://www.crummy.com/software/BeautifulSoup/zine/\n",
      "http://www.crummy.com/software/BeautifulSoup/bs4/\n",
      "http://www.crummy.com/software/BeautifulSoup/bs4/download/\n",
      "https://libraries.io/pypi/beautifulsoup4\n",
      "https://packaging.python.org/guides/analyzing-pypi-package-downloads/\n",
      "mailto:leonardr@segfault.org\n",
      "/user/leonard/\n",
      "/search/?c=Development+Status+%3A%3A+5+-+Production%2FStable\n",
      "/search/?c=Intended+Audience+%3A%3A+Developers\n",
      "/search/?c=License+%3A%3A+OSI+Approved+%3A%3A+MIT+License\n",
      "/search/?c=Programming+Language+%3A%3A+Python\n",
      "/search/?c=Programming+Language+%3A%3A+Python+%3A%3A+2.7\n",
      "/search/?c=Programming+Language+%3A%3A+Python+%3A%3A+3\n",
      "/search/?c=Topic+%3A%3A+Software+Development+%3A%3A+Libraries+%3A%3A+Python+Modules\n",
      "/search/?c=Topic+%3A%3A+Text+Processing+%3A%3A+Markup+%3A%3A+HTML\n",
      "/search/?c=Topic+%3A%3A+Text+Processing+%3A%3A+Markup+%3A%3A+SGML\n",
      "/search/?c=Topic+%3A%3A+Text+Processing+%3A%3A+Markup+%3A%3A+XML\n",
      "/help/#project-release-notifications\n",
      "/rss/project/beautifulsoup4/releases.xml\n",
      "/project/beautifulsoup4/4.9.3/\n",
      "/project/beautifulsoup4/4.9.2/\n",
      "/project/beautifulsoup4/4.9.1/\n",
      "/project/beautifulsoup4/4.9.0/\n",
      "/project/beautifulsoup4/4.8.2/\n",
      "/project/beautifulsoup4/4.8.1/\n",
      "/project/beautifulsoup4/4.8.0/\n",
      "/project/beautifulsoup4/4.7.1/\n",
      "/project/beautifulsoup4/4.7.0/\n",
      "/project/beautifulsoup4/4.6.3/\n",
      "/project/beautifulsoup4/4.6.2/\n",
      "/project/beautifulsoup4/4.6.1/\n",
      "/project/beautifulsoup4/4.6.0/\n",
      "/project/beautifulsoup4/4.5.3/\n",
      "/project/beautifulsoup4/4.5.2/\n",
      "/project/beautifulsoup4/4.5.1/\n",
      "/project/beautifulsoup4/4.5.0/\n",
      "/project/beautifulsoup4/4.4.1/\n",
      "/project/beautifulsoup4/4.4.0/\n",
      "/project/beautifulsoup4/4.3.2/\n",
      "/project/beautifulsoup4/4.3.1/\n",
      "/project/beautifulsoup4/4.3.0/\n",
      "/project/beautifulsoup4/4.2.1/\n",
      "/project/beautifulsoup4/4.2.0/\n",
      "/project/beautifulsoup4/4.1.3/\n",
      "/project/beautifulsoup4/4.1.2/\n",
      "/project/beautifulsoup4/4.1.1/\n",
      "/project/beautifulsoup4/4.1.0/\n",
      "/project/beautifulsoup4/4.0.5/\n",
      "/project/beautifulsoup4/4.0.4/\n",
      "/project/beautifulsoup4/4.0.3/\n",
      "/project/beautifulsoup4/4.0.2/\n",
      "/project/beautifulsoup4/4.0.1/\n",
      "https://packaging.python.org/installing/\n",
      "https://files.pythonhosted.org/packages/1c/d9/8c507915ff962b9e854b477b203c171074f59cce9465dac9f71c2b57ebd6/beautifulsoup4-4.9.3-py2-none-any.whl\n",
      "#copy-hash-modal-4eac8198-7a89-4b1f-b958-6169c4c31064\n",
      "https://files.pythonhosted.org/packages/d1/41/e6495bd7d3781cee623ce23ea6ac73282a373088fcd0ddc809a047b18eae/beautifulsoup4-4.9.3-py3-none-any.whl\n",
      "#copy-hash-modal-42de64e2-4880-4f44-9e64-4baebd680e7e\n",
      "https://files.pythonhosted.org/packages/6b/c3/d31704ae558dcca862e4ee8e8388f357af6c9d9acb0cad4ba0fbbd350d9a/beautifulsoup4-4.9.3.tar.gz\n",
      "#copy-hash-modal-c16f2c31-e33c-4cdc-8a08-e803998bb5b4\n",
      "#modal-close\n",
      "https://pip.pypa.io/en/stable/reference/pip_install/#hash-checking-mode\n",
      "#modal-close\n",
      "#modal-close\n",
      "https://pip.pypa.io/en/stable/reference/pip_install/#hash-checking-mode\n",
      "#modal-close\n",
      "#modal-close\n",
      "https://pip.pypa.io/en/stable/reference/pip_install/#hash-checking-mode\n",
      "#modal-close\n",
      "https://packaging.python.org/installing/\n",
      "https://packaging.python.org/tutorials/packaging-projects/\n",
      "https://packaging.python.org/\n",
      "/help/\n",
      "https://twitter.com/PyPI\n",
      "https://dtdg.co/pypi\n",
      "https://www.python.org/dev/peps/pep-0541/\n",
      "/sponsors/\n",
      "/help/#feedback\n",
      "https://github.com/pypa/warehouse\n",
      "https://hosted.weblate.org/projects/pypa/warehouse/\n",
      "https://github.com/pypa/warehouse/graphs/contributors\n",
      "https://github.com/pypa/.github/blob/main/CODE_OF_CONDUCT.md\n",
      "/security/\n",
      "https://www.python.org/privacy/\n",
      "/policy/terms-of-use/\n",
      "https://status.python.org/\n",
      "https://donate.pypi.org\n",
      "https://www.python.org/psf/\n",
      "/sitemap/\n",
      "https://www.pingdom.com/\n",
      "https://cloud.google.com/\n",
      "https://getsentry.com/for/python\n",
      "https://aws.amazon.com/\n",
      "https://www.datadoghq.com/\n",
      "https://www.fastly.com/\n",
      "https://www.digicert.com/\n",
      "https://statuspage.io\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = input('Enter -')\n",
    "html = urllib.request.urlopen(url).read() # open and read url\n",
    "soup = BeautifulSoup(html, 'html.parser') # parse with html parser\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    print(tag.get('href', None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment - Scraping Numbers from HTML using BeautifulSoup\n",
    "\n",
    "In this assignment you will write a Python program similar to http://www.py4e.com/code3/urllink2.py. The program will use urllib to read the HTML from the data files below, and parse the data, extracting numbers and compute the sum of the numbers in the file.\n",
    "\n",
    "We provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment.\n",
    "\n",
    "Sample data: http://py4e-data.dr-chuck.net/comments_42.html (Sum=2553)\n",
    "\n",
    "Actual data: http://py4e-data.dr-chuck.net/comments_1088549.html (Sum ends with 56\n",
    "\n",
    "You do not need to save these files to your folder since your program will read the data directly from the URL. Note: Each student will have a distinct data url for the assignment - so only use your own data url for analysis.\n",
    "\n",
    "Data Format\n",
    "\n",
    "The file is a table of names and comment counts. You can ignore most of the data in the file except for lines like the following:\n",
    "\n",
    "```\n",
    "<tr><td>Modu</td><td><span class=\"comments\">90</span></td></tr>\n",
    "<tr><td>Kenzie</td><td><span class=\"comments\">88</span></td></tr>\n",
    "<tr><td>Hubert</td><td><span class=\"comments\">87</span></td></tr>\n",
    "```\n",
    "\n",
    "You are to find all the <span> tags in the file and pull out the numbers from the tag and sum the numbers.\n",
    "Look at the sample code provided. It shows how to find all of a certain kind of tag, loop through the tags and extract the various aspects of the tags.\n",
    "    \n",
    "```\n",
    "...\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "   # Look at the parts of a tag\n",
    "   print 'TAG:',tag\n",
    "   print 'URL:',tag.get('href', None)\n",
    "   print 'Contents:',tag.contents[0]\n",
    "   print 'Attrs:',tag.attrs\n",
    "```\n",
    "You need to adjust this code to look for span tags and pull out the text content of the span tag, convert them to integers and add them up to complete the assignment.\n",
    "    \n",
    "Sample Execution\n",
    "\n",
    "```\n",
    "$ python3 solution.py\n",
    "Enter - http://py4e-data.dr-chuck.net/comments_42.html\n",
    "Count 50\n",
    "Sum 2...\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter -  http://py4e-data.dr-chuck.net/comments_1088549.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count 50\n",
      "Sum 2756\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "#print(soup)\n",
    "\n",
    "count = 0\n",
    "sum = 0\n",
    "\n",
    "# Retrieve all of the span tags\n",
    "tags = soup('span')\n",
    "for tag in tags:\n",
    "    # Look at the parts of a tag\n",
    "    #print('TAG:', tag)\n",
    "    #print('Contents:', tag.contents[0])\n",
    "    \n",
    "    # convert to integer\n",
    "    num = int(tag.contents[0])\n",
    "    count += 1\n",
    "    sum += num\n",
    "    \n",
    "print(\"Count\", count)\n",
    "print(\"Sum\", sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "<title>Welcome to the comments assignment from www.py4e.com</title>\n",
      "</head>\n",
      "<body>\n",
      "<h1>This file contains the sample data for testing</h1>\n",
      "\n",
      "<table border=\"2\">\n",
      "<tr>\n",
      "<td>Name</td><td>Comments</td>\n",
      "</tr>\n",
      "<tr><td>Romina</td><td><span class=\"comments\">97</span></td></tr>\n",
      "<tr><td>Laurie</td><td><span class=\"comments\">97</span></td></tr>\n",
      "<tr><td>Bayli</td><td><span class=\"comments\">90</span></td></tr>\n",
      "<tr><td>Siyona</td><td><span class=\"comments\">90</span></td></tr>\n",
      "<tr><td>Taisha</td><td><span class=\"comments\">88</span></td></tr>\n",
      "<tr><td>Alanda</td><td><span class=\"comments\">87</span></td></tr>\n",
      "<tr><td>Ameelia</td><td><span class=\"comments\">87</span></td></tr>\n",
      "<tr><td>Prasheeta</td><td><span class=\"comments\">80</span></td></tr>\n",
      "<tr><td>Asif</td><td><span class=\"comments\">79</span></td></tr>\n",
      "<tr><td>Risa</td><td><span class=\"comments\">79</span></td></tr>\n",
      "<tr><td>Zi</td><td><span class=\"comments\">78</span></td></tr>\n",
      "<tr><td>Danyil</td><td><span class=\"comments\">76</span></td></tr>\n",
      "<tr><td>Ediomi</td><td><span class=\"comments\">76</span></td></tr>\n",
      "<tr><td>Barry</td><td><span class=\"comments\">72</span></td></tr>\n",
      "<tr><td>Lance</td><td><span class=\"comments\">72</span></td></tr>\n",
      "<tr><td>Hattie</td><td><span class=\"comments\">66</span></td></tr>\n",
      "<tr><td>Mathu</td><td><span class=\"comments\">66</span></td></tr>\n",
      "<tr><td>Bowie</td><td><span class=\"comments\">65</span></td></tr>\n",
      "<tr><td>Samara</td><td><span class=\"comments\">65</span></td></tr>\n",
      "<tr><td>Uchenna</td><td><span class=\"comments\">64</span></td></tr>\n",
      "<tr><td>Shauni</td><td><span class=\"comments\">61</span></td></tr>\n",
      "<tr><td>Georgia</td><td><span class=\"comments\">61</span></td></tr>\n",
      "<tr><td>Rivan</td><td><span class=\"comments\">59</span></td></tr>\n",
      "<tr><td>Kenan</td><td><span class=\"comments\">58</span></td></tr>\n",
      "<tr><td>Hassan</td><td><span class=\"comments\">57</span></td></tr>\n",
      "<tr><td>Isma</td><td><span class=\"comments\">57</span></td></tr>\n",
      "<tr><td>Samanthalee</td><td><span class=\"comments\">54</span></td></tr>\n",
      "<tr><td>Alexa</td><td><span class=\"comments\">51</span></td></tr>\n",
      "<tr><td>Caine</td><td><span class=\"comments\">49</span></td></tr>\n",
      "<tr><td>Grady</td><td><span class=\"comments\">47</span></td></tr>\n",
      "<tr><td>Anne</td><td><span class=\"comments\">40</span></td></tr>\n",
      "<tr><td>Rihan</td><td><span class=\"comments\">38</span></td></tr>\n",
      "<tr><td>Alexei</td><td><span class=\"comments\">37</span></td></tr>\n",
      "<tr><td>Indie</td><td><span class=\"comments\">36</span></td></tr>\n",
      "<tr><td>Rhuairidh</td><td><span class=\"comments\">36</span></td></tr>\n",
      "<tr><td>Annoushka</td><td><span class=\"comments\">32</span></td></tr>\n",
      "<tr><td>Kenzi</td><td><span class=\"comments\">25</span></td></tr>\n",
      "<tr><td>Shahd</td><td><span class=\"comments\">24</span></td></tr>\n",
      "<tr><td>Irvine</td><td><span class=\"comments\">22</span></td></tr>\n",
      "<tr><td>Carys</td><td><span class=\"comments\">21</span></td></tr>\n",
      "<tr><td>Skye</td><td><span class=\"comments\">19</span></td></tr>\n",
      "<tr><td>Atiya</td><td><span class=\"comments\">18</span></td></tr>\n",
      "<tr><td>Rohan</td><td><span class=\"comments\">18</span></td></tr>\n",
      "<tr><td>Nuala</td><td><span class=\"comments\">14</span></td></tr>\n",
      "<tr><td>Maram</td><td><span class=\"comments\">12</span></td></tr>\n",
      "<tr><td>Carlo</td><td><span class=\"comments\">12</span></td></tr>\n",
      "<tr><td>Japleen</td><td><span class=\"comments\">9</span></td></tr>\n",
      "<tr><td>Breeanna</td><td><span class=\"comments\">7</span></td></tr>\n",
      "<tr><td>Zaaine</td><td><span class=\"comments\">3</span></td></tr>\n",
      "<tr><td>Inika</td><td><span class=\"comments\">2</span></td></tr>\n",
      "</table>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "fhand = urllib.request.urlopen('http://py4e-data.dr-chuck.net/comments_42.html')\n",
    "for line in fhand:\n",
    "    print(line.decode().strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment - Following Links in HTML Using BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter count:  4\n",
      "Enter position:  3\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = 'http://py4e-data.dr-chuck.net/known_by_Fikret.html'\n",
    "#url = input('Enter URL: ')\n",
    "count = input('Enter count: ')\n",
    "\n",
    "try:\n",
    "    icount = int(count)\n",
    "except:\n",
    "    print(\"Not a valid number!\")\n",
    "    quit()\n",
    "    \n",
    "pos = input('Enter position: ')\n",
    "\n",
    "try:\n",
    "    ipos = int(pos)\n",
    "except:\n",
    "    print(\"Not a valid number!\")\n",
    "    quit()\n",
    "        \n",
    "def follow(url, pos):\n",
    "    position = 0\n",
    "    html = urllib.request.urlopen(url, context=ctx).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Retrieve all of the a tags\n",
    "    tags = soup('a')\n",
    "    for tag in tags:\n",
    "        if position == 2:\n",
    "            foundUrl = tag.get('href', None)\n",
    "            print('Retrieving:', foundUrl)\n",
    "            return url\n",
    "        position += 1\n",
    "        \n",
    "# repeat this process 'count' times\n",
    "for i in range(icount, 3):\n",
    "    follow(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter URL:  http://py4e-data.dr-chuck.net/known_by_Malisa.html\n",
      "Enter count:  7\n",
      "Enter position:  18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Malisa.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Armelle.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Vuyolwethu.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Keigan.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Safi.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Atlanta.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Roxie.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Eliyah.html\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "#url = 'http://py4e-data.dr-chuck.net/known_by_Fikret.html'\n",
    "url = input('Enter URL: ')\n",
    "count = input('Enter count: ')\n",
    "\n",
    "try:\n",
    "    icount = int(count)\n",
    "except:\n",
    "    print(\"Not a valid number!\")\n",
    "    quit()\n",
    "\n",
    "pos = input('Enter position: ')\n",
    "\n",
    "try:\n",
    "    ipos = int(pos)\n",
    "except:\n",
    "    print(\"Not a valid number!\")\n",
    "    quit()\n",
    "\n",
    "def follow(url, count, pos):\n",
    "    # always print the first given url\n",
    "    print('Retrieving:', url)\n",
    "\n",
    "    # for-loop iterates count times\n",
    "    for i in range(count):\n",
    "        position = 0 # track position when looping thru tags, start at 0 means targetted at pos-1\n",
    "        html = urllib.request.urlopen(url, context=ctx).read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Retrieve all of the a tags\n",
    "        tags = soup('a')\n",
    "        for tag in tags:\n",
    "            # only if we are at specified position, then we retrieve that url\n",
    "            if position == pos - 1:\n",
    "                foundUrl = tag.get('href', None)\n",
    "                url = foundUrl # update the new url to go to next\n",
    "                print('Retrieving:', foundUrl)\n",
    "            # if not at pos-1, then we increment position to continue looking\n",
    "            position += 1\n",
    "\n",
    "# call the method\n",
    "follow(url, icount, ipos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 - Chapter 13 - Parsing XML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Chuck\n",
      "Attr: yes\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "data = '''<person>\n",
    "<name>Chuck</name>\n",
    "<phone type=\"intl\">+1 734 303 4456</phone>\n",
    "<email hide=\"yes\"/> \n",
    "</person>'''\n",
    "\n",
    "tree = ET.fromstring(data) \n",
    "print('Name:',tree.find('name').text) \n",
    "print('Attr:',tree.find('email').get('hide')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User count: 2\n",
      "Name Chuck\n",
      "Id 001\n",
      "Name 2\n",
      "Name Brent\n",
      "Id 009\n",
      "Name 7\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "input = '''<stuff>\n",
    "<users>\n",
    "    <user x=\"2\">\n",
    "        <id>001</id>\n",
    "        <name>Chuck</name>\n",
    "    </user>\n",
    "    <user x=\"7\">\n",
    "        <id>009</id>\n",
    "        <name>Brent</name>\n",
    "    </user>\n",
    "</users>\n",
    "</stuff>'''\n",
    "\n",
    "stuff = ET.fromstring(input) \n",
    "lst = stuff.findall('users/user') # find all user tag under users\n",
    "print('User count:', len(lst))\n",
    "for item in lst:\n",
    "    print('Name', item.find('name').text)\n",
    "    print('Id', item.find('id').text)\n",
    "    print('Name', item.get('x'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment - Extracting Data from XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter location: http://py4e-data.dr-chuck.net/comments_1088551.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving http://py4e-data.dr-chuck.net/comments_1088551.xml\n",
      "Count: 50\n",
      "Sum 2630\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import xml.etree.ElementTree as ET\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "#url = 'http://py4e-data.dr-chuck.net/comments_42.xml'\n",
    "url = input('Enter location:')\n",
    "print('Retrieving', url)\n",
    "response = urllib.request.urlopen(url).read()\n",
    "tree = ET.fromstring(response)\n",
    "\n",
    "lst = tree.findall('comments/comment') # find all comment under comments\n",
    "#lst = tree.findall('.//count') # or use XPath selector stirng to look thru entire XML tree for tag 'count'\n",
    "print('Count:', len(lst))\n",
    "\n",
    "sum = 0\n",
    "\n",
    "for item in lst:\n",
    "    #print('Comment name', item.find('name').text)\n",
    "    #print('Comment count', item.find('count').text)\n",
    "    icount = int(item.find('count').text)\n",
    "    sum += icount\n",
    "print('Sum', sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 - Chapter 12.5 - JavaScript Object Notation (JSON)\n",
    "\n",
    "- Aka in Python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Chuck\n",
      "Hide: {'hide': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "data = '''\n",
    "{\n",
    "    \"name\": \"Chuck\",\n",
    "    \"phone\": {\n",
    "        \"type\": \"intl\",\n",
    "        \"number\": \"+1 734 303 4456\"\n",
    "    },\n",
    "    \"email\":{\n",
    "        \"hide\": \"yes\"\n",
    "    }\n",
    "}'''\n",
    "\n",
    "info = json.loads(data)\n",
    "print('Name:', info[\"name\"])\n",
    "print('Hide:', info[\"email\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
