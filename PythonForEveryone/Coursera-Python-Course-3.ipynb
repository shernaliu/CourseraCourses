{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 - Chapter 11.1 - Regular Expressions\n",
    "## Understanding Regular Expressions\n",
    "\n",
    "Refer to [Regular Expression Quick Guide](https://www.py4e.com/html3/11-regex)\n",
    "\n",
    "`^` Matches the beginning of the line.\n",
    "\n",
    "`$` Matches the end of the line.\n",
    "\n",
    "`.` Matches any character (a wildcard).\n",
    "\n",
    "`\\s` Matches a whitespace character.\n",
    "\n",
    "`\\S` Matches a non-whitespace character (opposite of \\s).\n",
    "\n",
    "`*` Applies to the immediately preceding character(s) and indicates to match zero or more times.\n",
    "\n",
    "`*?` Applies to the immediately preceding character(s) and indicates to match zero or more times in “non-greedy mode”.\n",
    "\n",
    "`+` Applies to the immediately preceding character(s) and indicates to match one or more times.\n",
    "\n",
    "`+?` Applies to the immediately preceding character(s) and indicates to match one or more times in “non-greedy mode”.\n",
    "\n",
    "`?` Applies to the immediately preceding character(s) and indicates to match zero or one time.\n",
    "\n",
    "`??` Applies to the immediately preceding character(s) and indicates to match zero or one time in “non-greedy mode”.\n",
    "\n",
    "`[aeiou]` Matches a single character as long as that character is in the specified set. In this example, it would match “a”, “e”, “i”, “o”, or “u”, but no other characters.\n",
    "\n",
    "`[a-z0-9]` You can specify ranges of characters using the minus sign. This example is a single character that must be a lowercase letter or a digit.\n",
    "\n",
    "`[^A-Za-z]` When the first character in the set notation is a caret, it inverts the logic. This example matches a single character that is anything other than an uppercase or lowercase letter.\n",
    "\n",
    "`( )` When parentheses are added to a regular expression, they are ignored for the purpose of matching, but allow you to extract a particular subset of the matched string rather than the whole string when using findall().\n",
    "\n",
    "`\\b` Matches the empty string, but only at the start or end of a word.\n",
    "\n",
    "`\\B` Matches the empty string, but not at the start or end of a word.\n",
    "\n",
    "`\\d` Matches any decimal digit; equivalent to the set [0-9].\n",
    "\n",
    "`\\D` Matches any non-digit character; equivalent to the set [^0-9]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `re.search()` like `find()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: stephen.marquard@uct.ac.za\n",
      "From: louis@media.berkeley.edu\n",
      "From: zqian@umich.edu\n",
      "From: rjlowe@iupui.edu\n",
      "From: zqian@umich.edu\n",
      "From: rjlowe@iupui.edu\n",
      "From: cwen@iupui.edu\n",
      "From: cwen@iupui.edu\n",
      "From: gsilver@umich.edu\n",
      "From: gsilver@umich.edu\n",
      "From: zqian@umich.edu\n",
      "From: gsilver@umich.edu\n",
      "From: wagnermr@iupui.edu\n",
      "From: zqian@umich.edu\n",
      "From: antranig@caret.cam.ac.uk\n",
      "From: gopal.ramasammycook@gmail.com\n",
      "From: david.horwitz@uct.ac.za\n",
      "From: david.horwitz@uct.ac.za\n",
      "From: david.horwitz@uct.ac.za\n",
      "From: david.horwitz@uct.ac.za\n",
      "From: stephen.marquard@uct.ac.za\n",
      "From: louis@media.berkeley.edu\n",
      "From: louis@media.berkeley.edu\n",
      "From: ray@media.berkeley.edu\n",
      "From: cwen@iupui.edu\n",
      "From: cwen@iupui.edu\n",
      "From: cwen@iupui.edu\n"
     ]
    }
   ],
   "source": [
    "hand = open('mbox-short.txt')\n",
    "for line in hand:\n",
    "    line = line.rstrip()\n",
    "    if line.find('From:') >= 0:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: stephen.marquard@uct.ac.za\n",
      "From: louis@media.berkeley.edu\n",
      "From: zqian@umich.edu\n",
      "From: rjlowe@iupui.edu\n",
      "From: zqian@umich.edu\n",
      "From: rjlowe@iupui.edu\n",
      "From: cwen@iupui.edu\n",
      "From: cwen@iupui.edu\n",
      "From: gsilver@umich.edu\n",
      "From: gsilver@umich.edu\n",
      "From: zqian@umich.edu\n",
      "From: gsilver@umich.edu\n",
      "From: wagnermr@iupui.edu\n",
      "From: zqian@umich.edu\n",
      "From: antranig@caret.cam.ac.uk\n",
      "From: gopal.ramasammycook@gmail.com\n",
      "From: david.horwitz@uct.ac.za\n",
      "From: david.horwitz@uct.ac.za\n",
      "From: david.horwitz@uct.ac.za\n",
      "From: david.horwitz@uct.ac.za\n",
      "From: stephen.marquard@uct.ac.za\n",
      "From: louis@media.berkeley.edu\n",
      "From: louis@media.berkeley.edu\n",
      "From: ray@media.berkeley.edu\n",
      "From: cwen@iupui.edu\n",
      "From: cwen@iupui.edu\n",
      "From: cwen@iupui.edu\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "hand = open('mbox-short.txt')\n",
    "for line in hand:\n",
    "    line = line.rstrip()\n",
    "    if re.search('From:', line):\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wild-Card Characters\n",
    "- The dot character matches any character\n",
    "- If you add the asterisk character, the character is \"any number of times\"\n",
    "\n",
    "Use `^X.*:`\n",
    "\n",
    "For this example:\n",
    "\n",
    "```\n",
    "X-Sieve: CMU Sieve 2.3\n",
    "X-DSPAM-Result: Innocent\n",
    "X-DSPAM-Confidence: 0.8475\n",
    "X-Content-Type-Message-Body: text/plain\n",
    "```\n",
    "\n",
    "## Fine-Tuning Your Match\n",
    "\n",
    "Use `^X-\\S+:`\n",
    "\n",
    "For this example:\n",
    "\n",
    "```\n",
    "X-Sieve: CMU Sieve 2.3\n",
    "X-DSPAM-Result: Innocent\n",
    "X-Plane is behind schedule: two weeks\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching and Extracting Data\n",
    "- `re.search()` returns a T/F depending on whether the string matches the regular expression\n",
    "- If we want to extract the matching strings, use `re.findall()`\n",
    "- `[0-9]+` means one or more digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '6', '17']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# returns a list of matching numbers\n",
    "x = 'My 2 favourite numbers are 6 and 17'\n",
    "y = re.findall('[0-9]+',x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# there are no upper case vowels in the string x, returns empty list\n",
    "y = re.findall('[AEIOU]+', x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning: Greedy Matching\n",
    "- The repeat charracters `(*` and `+)` push outward in both directions (greedy) to match the largest possible string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From: Using the :']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "x = 'From: Using the : character'\n",
    "y = re.findall('^F.+:', x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Greedy Matching\n",
    "- Not all regular expression repeat codes are greedy!\n",
    "- If you add a `?` character, the `+` and `*` chill out a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From:']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "x = 'From: Using the : character'\n",
    "y = re.findall('^F.+?:', x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning String Extraction\n",
    "- You can redefine the match for `re.findall()` and separately determine which portion of the match is to be extracted by using parenthesis.\n",
    "- `\\S+` means at least one non-whitespace character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stephen.marquard@uct.ac.za']\n"
     ]
    }
   ],
   "source": [
    "x = 'From stephen.marquard@uct.ac.za Sat Jun  5 09:14:16 2008'\n",
    "y = re.findall('\\S+@\\S+', x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stephen.marquard@uct.ac.za']\n"
     ]
    }
   ],
   "source": [
    "# Match the 'From email@email' but extract just the email portion\n",
    "x = 'From stephen.marquard@uct.ac.za Sat Jun  5 09:14:16 2008'\n",
    "y = re.findall('From (\\S+@\\S+)', x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Double Split Pattern (Old)\n",
    "- Sometimes we split a line one way, then grab one of t he pieces of the line and split that piece again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From', 'stephen.marquard@uct.ac.za', 'Sat', 'Jun', '5', '09:14:16', '2008']\n",
      "stephen.marquard@uct.ac.za\n",
      "uct.ac.za\n"
     ]
    }
   ],
   "source": [
    "line = 'From stephen.marquard@uct.ac.za Sat Jun  5 09:14:16 2008'\n",
    "\n",
    "words = line.split()\n",
    "print(words)\n",
    "\n",
    "email = words[1]\n",
    "print(email)\n",
    "\n",
    "pieces = email.split('@')\n",
    "print(pieces[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Double Split Pattern (Regex Version)\n",
    "`@([^ ]*)` look thru the string until you find an at sign\n",
    "\n",
    "`[^ ]` match non-blank character\n",
    "\n",
    "`*` match many of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uct.ac.za']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "line = 'From stephen.marquard@uct.ac.za Sat Jun  5 09:14:16 2008'\n",
    "y = re.findall('@([^ ]*)', line)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Double Split Pattern (Even Cooler Regex Version)\n",
    "`^From .*@([^ ]*)` \n",
    "\n",
    "Start at the beginning of the line, look for the string 'From\n",
    "\n",
    "look thru the string until you find an at sign\n",
    "\n",
    "`[^ ]` match non-blank character\n",
    "\n",
    "`*` match many of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uct.ac.za']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "line = 'From stephen.marquard@uct.ac.za Sat Jun  5 09:14:16 2008'\n",
    "y = re.findall('^From .*@([^ ]*)', line)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spam Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum: 0.9907\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "hand = open('mbox-short.txt')\n",
    "numlist = list()\n",
    "for line in hand:\n",
    "    line = line.rstrip()\n",
    "    stuff = re.findall('X-DSPAM-Confidence: ([0-9.]+)', line)\n",
    "    # stuff returns a list of numbers\n",
    "    # if there are more than 1 item in list, means something wrong\n",
    "    if len(stuff) != 1 : continue\n",
    "    num = float(stuff[0])\n",
    "    numlist.append(num)\n",
    "print('Maximum:', max(numlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escape Character\n",
    "- If you want a special regular expression character to just behave normally (most of the time) you prefix it with a '\\'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$10.00']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "x = 'We just received $10.00 for cookies.'\n",
    "y = re.findall('\\$[0-9.]+', x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uct.ac.za']\n"
     ]
    }
   ],
   "source": [
    "# quiz\n",
    "import re\n",
    "line = 'From stephen.marquard@uct.ac.za Sat Jun  5 09:14:16 2008'\n",
    "y = re.findall('@(\\S+)', line)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From: Using the :']\n"
     ]
    }
   ],
   "source": [
    "x = 'From: Using the : character'\n",
    "y = re.findall('^F.+:', x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stephen.marquard@uct.ac.za']\n"
     ]
    }
   ],
   "source": [
    "# quiz\n",
    "import re\n",
    "line = 'From stephen.marquard@uct.ac.za Sat Jun  5 09:14:16 2008'\n",
    "y = re.findall('\\S+?@\\S+', line)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Extracting Data With Regular Expressions\n",
    "\n",
    "Finding Numbers in a Haystack\n",
    "\n",
    "In this assignment you will read through and parse a file with text and numbers. You will extract all the numbers in the file and compute the sum of the numbers.\n",
    "\n",
    "Data Files\n",
    "We provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment.\n",
    "\n",
    "Sample data: http://py4e-data.dr-chuck.net/regex_sum_42.txt (There are 90 values with a sum=445833)\n",
    "\n",
    "Actual data: http://py4e-data.dr-chuck.net/regex_sum_1088547.txt (There are 67 values and the sum ends with 236)\n",
    "\n",
    "These links open in a new window. Make sure to save the file into the same folder as you will be writing your Python program. Note: Each student will have a distinct data file for the assignment - so only use your own data file for analysis.\n",
    "\n",
    "Data Format\n",
    "\n",
    "The file contains much of the text from the introduction of the textbook except that random numbers are inserted throughout the text. Here is a sample of the output you might see:\n",
    "\n",
    "```\n",
    "Why should you learn to write programs? 7746\n",
    "12 1929 8827\n",
    "Writing programs (or programming) is a very creative \n",
    "7 and rewarding activity.  You can write programs for \n",
    "many reasons, ranging from making your living to solving\n",
    "8837 a difficult data analysis problem to having fun to helping 128\n",
    "someone else solve a problem.  This book assumes that \n",
    "everyone needs to know how to program ...\n",
    "```\n",
    "The sum for the sample text above is 27486. The numbers can appear anywhere in the line. There can be any number of numbers in each line (including none).\n",
    "\n",
    "**Handling The Data**\n",
    "\n",
    "The basic outline of this problem is to read the file, look for integers using the re.findall(), looking for a regular expression of '[0-9]+' and then converting the extracted strings to integers and summing up the integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum: 318236\n"
     ]
    }
   ],
   "source": [
    "# import regex\n",
    "import re\n",
    "sum = 0\n",
    "\n",
    "# open the file, use try-except to handle exceptions gracefully\n",
    "try:\n",
    "    fh = open('regex_sum_1088547.txt')\n",
    "except:\n",
    "    print(\"Unable to open file!\")\n",
    "    quit()\n",
    "\n",
    "# read the file and use regular expressions to extract the numbers\n",
    "for line in fh:\n",
    "    line = line.rstrip()\n",
    "    extracted = re.findall('[0-9]+', line)\n",
    "     # if the extracted list of strings has length 0, means no numbers detected. Thus, skip (continue)\n",
    "    if len(extracted) == 0 : continue\n",
    "    #print(extracted)\n",
    "    \n",
    "    # for each number in the list of strings, parse to integers and sum them up\n",
    "    for number in extracted:\n",
    "        sum = sum + int(number)\n",
    "        \n",
    "print(\"Sum:\", sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Just for Fun\n",
    "There are a number of different ways to approach this problem. While we don't recommend trying to write the most compact code possible, it can sometimes be a fun exercise. Here is a a redacted version of two-line version of this program using list comprehension:\n",
    "\n",
    "```\n",
    "import re\n",
    "print( sum( [ ****** *** * in **********('[0-9]+',**************************.read()) ] ) )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 3 - Chapter 12 - Networked Technology\n",
    "## Hypertext Transfer Protocol (HTTP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\n",
      "Date: Fri, 04 Dec 2020 15:54:41 GMT\n",
      "Server: Apache/2.4.18 (Ubuntu)\n",
      "Last-Modified: Sat, 13 May 2017 11:22:22 GMT\n",
      "ETag: \"a7-54f6609245537\"\n",
      "Accept-Ranges: bytes\n",
      "Content-Length: 167\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\n",
      "Pragma: no-cache\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\n",
      "Connection: close\n",
      "Content-Type: text/plain\n",
      "\n",
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already s\n",
      "ick and pale with grief\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "# init new socket\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "# connect to stated URL at port 80\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "\n",
    "# encode the string to send aka convert Unicode to UTF-8\n",
    "# because strings in Python are in Unicode, but you need to send in UTF-8 bytes.\n",
    "cmd = 'GET http://data.pr4e.org/romeo.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "\n",
    "# send encoded string to server\n",
    "mysock.send(cmd)\n",
    "\n",
    "# keep looping, keep receiving & printing data until we hit end of transmission & break out of loop\n",
    "while True: \n",
    "    data = mysock.recv(512) # use receive method of socket to get data, receive up to 512 chars\n",
    "    # if we get no data, that means end of transmission\n",
    "    if (len(data) < 1):\n",
    "        break\n",
    "    print(data.decode())\n",
    "\n",
    "# close the socket once done\n",
    "mysock.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment - Understanding the Request / Response Cycle\n",
    "\n",
    "Exploring the HyperText Transport Protocol\n",
    "\n",
    "You are to retrieve the following document using the HTTP protocol in a way that you can examine the HTTP Response headers.\n",
    "\n",
    "http://data.pr4e.org/intro-short.txt\n",
    "\n",
    "There are three ways that you might retrieve this web page and look at the response headers:\n",
    "\n",
    "Preferred: Modify the socket1.py program to retrieve the above URL and print out the headers and data.\n",
    "\n",
    "Make sure to change the code to retrieve the above URL - the values are different for each URL.\n",
    "\n",
    "Open the URL in a web browser with a developer console or FireBug and manually examine the headers that are returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\n",
      "Date: Fri, 04 Dec 2020 15:54:41 GMT\n",
      "Server: Apache/2.4.18 (Ubuntu)\n",
      "Last-Modified: Sat, 13 May 2017 11:22:22 GMT\n",
      "ETag: \"1d3-54f6609240717\"\n",
      "Accept-Ranges: bytes\n",
      "Content-Length: 467\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\n",
      "Pragma: no-cache\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\n",
      "Connection: close\n",
      "Content-Type: text/plain\n",
      "\n",
      "Why should you learn to write programs?\n",
      "\n",
      "Writing programs (or programming) is a very creative \n",
      "and rewarding activity.  You can write programs\n",
      " for \n",
      "many reasons, ranging from making your living to solving\n",
      "a difficult data analysis problem to having fun to helping\n",
      "someone else solve a problem.  This book assumes that \n",
      "everyone needs to know how to program, and that once \n",
      "you know how to program you will figure out what you want \n",
      "to do with your newfound skills.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "# init new socket\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "# connect to stated URL at port 80\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "\n",
    "# encode the string to send aka convert Unicode to UTF-8\n",
    "# because strings in Python are in Unicode, but you need to send in UTF-8 bytes.\n",
    "cmd = 'GET http://data.pr4e.org/intro-short.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "\n",
    "# send encoded string to server\n",
    "mysock.send(cmd)\n",
    "\n",
    "# keep looping, keep receiving & printing data until we hit end of transmission & break out of loop\n",
    "while True: \n",
    "    data = mysock.recv(512) # use receive method of socket to get data, receive up to 512 chars\n",
    "    # if we get no data, that means end of transmission\n",
    "    if (len(data) < 1):\n",
    "        break\n",
    "    print(data.decode())\n",
    "\n",
    "# close the socket once done\n",
    "mysock.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 - Chapter 12 - Unicode Characters and Strings\n",
    "\n",
    "- ASCII (Mapping for numbers to characters)\n",
    "\n",
    "### Representing Simple Strings\n",
    "\n",
    "- Each character is represented by a number between 0 and 256 stored in 8 bits of memory.\n",
    "\n",
    "- We refer to \"8 bits of memory as a byte\" of memory (i.e. my disk drive contains 3 Terabytes of memory)\n",
    "\n",
    "- The `ord()` function tells us the numeric value of a simple ASCII character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "101\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# uppercase has lower ordinal than lowercase\n",
    "print(ord('H'))\n",
    "print(ord('e'))\n",
    "print(ord('\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Byte Characters\n",
    "\n",
    "- To represent the wide range of characters computers must handle, we represent characters with more than one byte\n",
    "\n",
    "    - UTF-16 - Fixed length - Two bytes\n",
    "    \n",
    "    - UTF-32 - Fixed length - Four bytes\n",
    "    \n",
    "    - UTF-8 - 1~4 bytes\n",
    "    \n",
    "      - Upwards compatible with ASCII\n",
    "      \n",
    "      - Automatic detection between ASCII and UTF-8\n",
    "      \n",
    "      - UTF-8 is recommended practice for encoding data to be exchanged between systems\n",
    "      \n",
    "- In Python, all strings are unicode.\n",
    "\n",
    "- When we talk to a network resource using sockets or talk to a database we have to encode and decode data (usually to UTF-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Strings to Bytes\n",
    "\n",
    "- When we talk to an external resource like a network socket,  we send bytes.\n",
    "\n",
    "- So, we need to encode Python 3 strings into a given character encoding.\n",
    "\n",
    "- When we read data from an external source, we must decode it based on the character set so it is properly represented in Python 3 as a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Web Pages using `urllib` in Python\n",
    "\n",
    "- We have a library that does all the socket work for us and make web pages look like a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "fhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "for line in fhand:\n",
    "    print(line.decode().strip())\n",
    "    \n",
    "counts = dict()\n",
    "for line in fhand:\n",
    "    words = line.decode().split()\n",
    "    for word in words:\n",
    "        counts[word] = counts.get(word, 0) + 1\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading web pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>The First Page</h1>\n",
      "<p>\n",
      "If you like, you can switch to the\n",
      "<a href=\"http://www.dr-chuck.com/page2.htm\">\n",
      "Second Page</a>.\n",
      "</p>\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "fhand = urllib.request.urlopen('http://www.dr-chuck.com/page1.htm')\n",
    "for line in fhand:\n",
    "    print(line.decode().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping\n",
    "\n",
    "- When a program or script pretends to be a browser and retrieves web pages, looks at those web pages, extracts information and then looks at more web pages\n",
    "\n",
    "- Search engines scrape web pages - we call this \"spidering the web\" or \"web crawling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another library: Beautiful Soup\n",
    "\n",
    "- Library to easily parse web pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sherna\\anaconda3\\lib\\site-packages (4.9.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sherna\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter - http://www.dr-chuck.com/page2.htm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page1.htm\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# url = 'http://www.dr-chuck.com/page2.htm'\n",
    "url = input('Enter -')\n",
    "html = urllib.request.urlopen(url).read() # open and read url\n",
    "soup = BeautifulSoup(html, 'html.parser') # parse with html parser\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    print(tag.get('href', None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment - Scraping Numbers from HTML using BeautifulSoup\n",
    "\n",
    "In this assignment you will write a Python program similar to http://www.py4e.com/code3/urllink2.py. The program will use urllib to read the HTML from the data files below, and parse the data, extracting numbers and compute the sum of the numbers in the file.\n",
    "\n",
    "We provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment.\n",
    "\n",
    "Sample data: http://py4e-data.dr-chuck.net/comments_42.html (Sum=2553)\n",
    "\n",
    "Actual data: http://py4e-data.dr-chuck.net/comments_1088549.html (Sum ends with 56\n",
    "\n",
    "You do not need to save these files to your folder since your program will read the data directly from the URL. Note: Each student will have a distinct data url for the assignment - so only use your own data url for analysis.\n",
    "\n",
    "Data Format\n",
    "\n",
    "The file is a table of names and comment counts. You can ignore most of the data in the file except for lines like the following:\n",
    "\n",
    "```\n",
    "<tr><td>Modu</td><td><span class=\"comments\">90</span></td></tr>\n",
    "<tr><td>Kenzie</td><td><span class=\"comments\">88</span></td></tr>\n",
    "<tr><td>Hubert</td><td><span class=\"comments\">87</span></td></tr>\n",
    "```\n",
    "\n",
    "You are to find all the <span> tags in the file and pull out the numbers from the tag and sum the numbers.\n",
    "Look at the sample code provided. It shows how to find all of a certain kind of tag, loop through the tags and extract the various aspects of the tags.\n",
    "    \n",
    "```\n",
    "...\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "   # Look at the parts of a tag\n",
    "   print 'TAG:',tag\n",
    "   print 'URL:',tag.get('href', None)\n",
    "   print 'Contents:',tag.contents[0]\n",
    "   print 'Attrs:',tag.attrs\n",
    "```\n",
    "You need to adjust this code to look for span tags and pull out the text content of the span tag, convert them to integers and add them up to complete the assignment.\n",
    "    \n",
    "Sample Execution\n",
    "\n",
    "```\n",
    "$ python3 solution.py\n",
    "Enter - http://py4e-data.dr-chuck.net/comments_42.html\n",
    "Count 50\n",
    "Sum 2...\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter -  http://py4e-data.dr-chuck.net/comments_42.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count 50\n",
      "Sum 2553\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "#print(soup)\n",
    "\n",
    "count = 0\n",
    "sum = 0\n",
    "\n",
    "# Retrieve all of the span tags\n",
    "tags = soup('span')\n",
    "for tag in tags:\n",
    "    # Look at the parts of a tag\n",
    "    #print('TAG:', tag)\n",
    "    #print('Contents:', tag.contents[0])\n",
    "    \n",
    "    # convert to integer\n",
    "    num = int(tag.contents[0])\n",
    "    count += 1\n",
    "    sum += num\n",
    "    \n",
    "print(\"Count\", count)\n",
    "print(\"Sum\", sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "<title>Welcome to the comments assignment from www.py4e.com</title>\n",
      "</head>\n",
      "<body>\n",
      "<h1>This file contains the sample data for testing</h1>\n",
      "\n",
      "<table border=\"2\">\n",
      "<tr>\n",
      "<td>Name</td><td>Comments</td>\n",
      "</tr>\n",
      "<tr><td>Romina</td><td><span class=\"comments\">97</span></td></tr>\n",
      "<tr><td>Laurie</td><td><span class=\"comments\">97</span></td></tr>\n",
      "<tr><td>Bayli</td><td><span class=\"comments\">90</span></td></tr>\n",
      "<tr><td>Siyona</td><td><span class=\"comments\">90</span></td></tr>\n",
      "<tr><td>Taisha</td><td><span class=\"comments\">88</span></td></tr>\n",
      "<tr><td>Alanda</td><td><span class=\"comments\">87</span></td></tr>\n",
      "<tr><td>Ameelia</td><td><span class=\"comments\">87</span></td></tr>\n",
      "<tr><td>Prasheeta</td><td><span class=\"comments\">80</span></td></tr>\n",
      "<tr><td>Asif</td><td><span class=\"comments\">79</span></td></tr>\n",
      "<tr><td>Risa</td><td><span class=\"comments\">79</span></td></tr>\n",
      "<tr><td>Zi</td><td><span class=\"comments\">78</span></td></tr>\n",
      "<tr><td>Danyil</td><td><span class=\"comments\">76</span></td></tr>\n",
      "<tr><td>Ediomi</td><td><span class=\"comments\">76</span></td></tr>\n",
      "<tr><td>Barry</td><td><span class=\"comments\">72</span></td></tr>\n",
      "<tr><td>Lance</td><td><span class=\"comments\">72</span></td></tr>\n",
      "<tr><td>Hattie</td><td><span class=\"comments\">66</span></td></tr>\n",
      "<tr><td>Mathu</td><td><span class=\"comments\">66</span></td></tr>\n",
      "<tr><td>Bowie</td><td><span class=\"comments\">65</span></td></tr>\n",
      "<tr><td>Samara</td><td><span class=\"comments\">65</span></td></tr>\n",
      "<tr><td>Uchenna</td><td><span class=\"comments\">64</span></td></tr>\n",
      "<tr><td>Shauni</td><td><span class=\"comments\">61</span></td></tr>\n",
      "<tr><td>Georgia</td><td><span class=\"comments\">61</span></td></tr>\n",
      "<tr><td>Rivan</td><td><span class=\"comments\">59</span></td></tr>\n",
      "<tr><td>Kenan</td><td><span class=\"comments\">58</span></td></tr>\n",
      "<tr><td>Hassan</td><td><span class=\"comments\">57</span></td></tr>\n",
      "<tr><td>Isma</td><td><span class=\"comments\">57</span></td></tr>\n",
      "<tr><td>Samanthalee</td><td><span class=\"comments\">54</span></td></tr>\n",
      "<tr><td>Alexa</td><td><span class=\"comments\">51</span></td></tr>\n",
      "<tr><td>Caine</td><td><span class=\"comments\">49</span></td></tr>\n",
      "<tr><td>Grady</td><td><span class=\"comments\">47</span></td></tr>\n",
      "<tr><td>Anne</td><td><span class=\"comments\">40</span></td></tr>\n",
      "<tr><td>Rihan</td><td><span class=\"comments\">38</span></td></tr>\n",
      "<tr><td>Alexei</td><td><span class=\"comments\">37</span></td></tr>\n",
      "<tr><td>Indie</td><td><span class=\"comments\">36</span></td></tr>\n",
      "<tr><td>Rhuairidh</td><td><span class=\"comments\">36</span></td></tr>\n",
      "<tr><td>Annoushka</td><td><span class=\"comments\">32</span></td></tr>\n",
      "<tr><td>Kenzi</td><td><span class=\"comments\">25</span></td></tr>\n",
      "<tr><td>Shahd</td><td><span class=\"comments\">24</span></td></tr>\n",
      "<tr><td>Irvine</td><td><span class=\"comments\">22</span></td></tr>\n",
      "<tr><td>Carys</td><td><span class=\"comments\">21</span></td></tr>\n",
      "<tr><td>Skye</td><td><span class=\"comments\">19</span></td></tr>\n",
      "<tr><td>Atiya</td><td><span class=\"comments\">18</span></td></tr>\n",
      "<tr><td>Rohan</td><td><span class=\"comments\">18</span></td></tr>\n",
      "<tr><td>Nuala</td><td><span class=\"comments\">14</span></td></tr>\n",
      "<tr><td>Maram</td><td><span class=\"comments\">12</span></td></tr>\n",
      "<tr><td>Carlo</td><td><span class=\"comments\">12</span></td></tr>\n",
      "<tr><td>Japleen</td><td><span class=\"comments\">9</span></td></tr>\n",
      "<tr><td>Breeanna</td><td><span class=\"comments\">7</span></td></tr>\n",
      "<tr><td>Zaaine</td><td><span class=\"comments\">3</span></td></tr>\n",
      "<tr><td>Inika</td><td><span class=\"comments\">2</span></td></tr>\n",
      "</table>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "fhand = urllib.request.urlopen('http://py4e-data.dr-chuck.net/comments_42.html')\n",
    "for line in fhand:\n",
    "    print(line.decode().strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment - Following Links in HTML Using BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter URL:  http://py4e-data.dr-chuck.net/known_by_Fikret.html\n",
      "Enter count:  4\n",
      "Enter position:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Fikret.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Montgomery.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Mhairade.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Butchi.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Anayah.html\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "#url = 'http://py4e-data.dr-chuck.net/known_by_Fikret.html'\n",
    "url = input('Enter URL: ')\n",
    "count = input('Enter count: ')\n",
    "\n",
    "try:\n",
    "    icount = int(count)\n",
    "except:\n",
    "    print(\"Not a valid number!\")\n",
    "    quit()\n",
    "\n",
    "pos = input('Enter position: ')\n",
    "\n",
    "try:\n",
    "    ipos = int(pos)\n",
    "except:\n",
    "    print(\"Not a valid number!\")\n",
    "    quit()\n",
    "\n",
    "def follow(url, count, pos):\n",
    "    # always print the first given url\n",
    "    print('Retrieving:', url)\n",
    "\n",
    "    # for-loop iterates count times\n",
    "    for i in range(count):\n",
    "        position = 0 # track position when looping thru tags, start at 0 means targetted at pos-1\n",
    "        html = urllib.request.urlopen(url, context=ctx).read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Retrieve all of the a tags\n",
    "        tags = soup('a')\n",
    "        for tag in tags:\n",
    "            # only if we are at specified position, then we retrieve that url\n",
    "            if position == pos - 1:\n",
    "                foundUrl = tag.get('href', None)\n",
    "                url = foundUrl # update the new url to go to next\n",
    "                print('Retrieving:', foundUrl)\n",
    "            # if not at pos-1, then we increment position to continue looking\n",
    "            position += 1\n",
    "\n",
    "# call the method\n",
    "follow(url, icount, ipos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 - Chapter 13 - Parsing XML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Chuck\n",
      "Attr: yes\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "data = '''<person>\n",
    "<name>Chuck</name>\n",
    "<phone type=\"intl\">+1 734 303 4456</phone>\n",
    "<email hide=\"yes\"/> \n",
    "</person>'''\n",
    "\n",
    "tree = ET.fromstring(data) \n",
    "print('Name:',tree.find('name').text) \n",
    "print('Attr:',tree.find('email').get('hide')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User count: 2\n",
      "Name Chuck\n",
      "Id 001\n",
      "Name 2\n",
      "Name Brent\n",
      "Id 009\n",
      "Name 7\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "input = '''<stuff>\n",
    "<users>\n",
    "    <user x=\"2\">\n",
    "        <id>001</id>\n",
    "        <name>Chuck</name>\n",
    "    </user>\n",
    "    <user x=\"7\">\n",
    "        <id>009</id>\n",
    "        <name>Brent</name>\n",
    "    </user>\n",
    "</users>\n",
    "</stuff>'''\n",
    "\n",
    "stuff = ET.fromstring(input) \n",
    "lst = stuff.findall('users/user') # find all user tag under users\n",
    "print('User count:', len(lst))\n",
    "for item in lst:\n",
    "    print('Name', item.find('name').text)\n",
    "    print('Id', item.find('id').text)\n",
    "    print('Name', item.get('x'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment - Extracting Data from XML\n",
    "\n",
    "In this assignment you will write a Python program somewhat similar to http://www.py4e.com/code3/geoxml.py. The program will prompt for a URL, read the XML data from that URL using urllib and then parse and extract the comment counts from the XML data, compute the sum of the numbers in the file.\n",
    "\n",
    "We provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment.\n",
    "\n",
    "Sample data: http://py4e-data.dr-chuck.net/comments_42.xml (Sum=2553)\n",
    "\n",
    "Actual data: http://py4e-data.dr-chuck.net/comments_1088551.xml (Sum ends with 30)\n",
    "\n",
    "You do not need to save these files to your folder since your program will read the data directly from the URL. Note: Each student will have a distinct data url for the assignment - so only use your own data url for analysis.\n",
    "\n",
    "Data Format and Approach\n",
    "\n",
    "The data consists of a number of names and comment counts in XML as follows:\n",
    "\n",
    "```\n",
    "<comment>\n",
    "  <name>Matthias</name>\n",
    "  <count>97</count>\n",
    "</comment>\n",
    "```\n",
    "\n",
    "You are to look through all the <comment> tags and find the <count> values sum the numbers. The closest sample code that shows how to parse XML is geoxml.py. But since the nesting of the elements in our data is different than the data we are parsing in that sample code you will have to make real changes to the code.\n",
    "To make the code a little simpler, you can use an XPath selector string to look through the entire tree of XML for any tag named 'count' with the following line of code:\n",
    "\n",
    "```\n",
    "counts = tree.findall('.//count')\n",
    "```\n",
    "    \n",
    "Take a look at the Python ElementTree documentation and look for the supported XPath syntax for details. You could also work from the top of the XML down to the comments node and then loop through the child nodes of the comments node.\n",
    "Sample Execution\n",
    "\n",
    "```\n",
    "$ python3 solution.py\n",
    "Enter location: http://py4e-data.dr-chuck.net/comments_42.xml\n",
    "Retrieving http://py4e-data.dr-chuck.net/comments_42.xml\n",
    "Retrieved 4189 characters\n",
    "Count: 50\n",
    "Sum: 2...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter location: http://py4e-data.dr-chuck.net/comments_42.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving http://py4e-data.dr-chuck.net/comments_42.xml\n",
      "Count: 50\n",
      "Sum 2553\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import xml.etree.ElementTree as ET\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "#url = 'http://py4e-data.dr-chuck.net/comments_42.xml'\n",
    "url = input('Enter location:')\n",
    "print('Retrieving', url)\n",
    "response = urllib.request.urlopen(url).read()\n",
    "tree = ET.fromstring(response)\n",
    "\n",
    "lst = tree.findall('comments/comment') # find all comment under comments\n",
    "#lst = tree.findall('.//count') # or use XPath selector stirng to look thru entire XML tree for tag 'count'\n",
    "print('Count:', len(lst))\n",
    "\n",
    "sum = 0\n",
    "\n",
    "for item in lst:\n",
    "    #print('Comment name', item.find('name').text)\n",
    "    #print('Comment count', item.find('count').text)\n",
    "    icount = int(item.find('count').text)\n",
    "    sum += icount\n",
    "print('Sum', sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 - Chapter 12.5 - JavaScript Object Notation (JSON)\n",
    "\n",
    "- Aka in Python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Chuck\n",
      "Hide: {'hide': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "data = '''\n",
    "{\n",
    "    \"name\": \"Chuck\",\n",
    "    \"phone\": {\n",
    "        \"type\": \"intl\",\n",
    "        \"number\": \"+1 734 303 4456\"\n",
    "    },\n",
    "    \"email\":{\n",
    "        \"hide\": \"yes\"\n",
    "    }\n",
    "}'''\n",
    "\n",
    "info = json.loads(data)\n",
    "print('Name:', info[\"name\"])\n",
    "print('Hide:', info[\"email\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Application Programming Interfaces (APIs)\n",
    "\n",
    "- Doesnt work cos requires Google API key (might be purchase)\n",
    "- alternative url found in discussion forums\n",
    "http://py4e-data.dr-chuck.net/json?address=Ann+Arbor%2C+MI&key=42\n",
    "\n",
    "- Input: Ann Arbor, MI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter location:  Ann Arbor, MI\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving http://py4e-data.dr-chuck.net/json?address=Ann+Arbor%2C+MI&key=42\n",
      "Retrieved 1736 characters\n",
      "lat 42.2808256 lng -83.7430378\n",
      "[{'address_components': [{'long_name': 'Ann Arbor', 'short_name': 'Ann Arbor', 'types': ['locality', 'political']}, {'long_name': 'Washtenaw County', 'short_name': 'Washtenaw County', 'types': ['administrative_area_level_2', 'political']}, {'long_name': 'Michigan', 'short_name': 'MI', 'types': ['administrative_area_level_1', 'political']}, {'long_name': 'United States', 'short_name': 'US', 'types': ['country', 'political']}], 'formatted_address': 'Ann Arbor, MI, USA', 'geometry': {'bounds': {'northeast': {'lat': 42.3239728, 'lng': -83.6758069}, 'southwest': {'lat': 42.222668, 'lng': -83.799572}}, 'location': {'lat': 42.2808256, 'lng': -83.7430378}, 'location_type': 'APPROXIMATE', 'viewport': {'northeast': {'lat': 42.3239728, 'lng': -83.6758069}, 'southwest': {'lat': 42.222668, 'lng': -83.799572}}}, 'place_id': 'ChIJMx9D1A2wPIgR4rXIhkb5Cds', 'types': ['locality', 'political']}]\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import json\n",
    "\n",
    "serviceUrl = 'http://py4e-data.dr-chuck.net/json?'\n",
    "\n",
    "# DISABLED LOOP COS KERNAL WILL WAIT FOR INPUT IF U RESTART AND RUN ALL\n",
    "#while True:\n",
    "address = input('Enter location: ')\n",
    "#if len(address) < 1: break\n",
    "\n",
    "url = serviceUrl + urllib.parse.urlencode({'address': address, 'key': 42})\n",
    "\n",
    "print('Retrieving', url)\n",
    "uh = urllib.request.urlopen(url)\n",
    "data = uh.read().decode()\n",
    "print('Retrieved', len(data), 'characters')\n",
    "\n",
    "try:\n",
    "    js = json.loads(data)\n",
    "except:\n",
    "    js = None\n",
    "\n",
    "if not js or 'status' not in js or js['status'] != 'OK':\n",
    "    print('=== Failre To Retrieve ====')\n",
    "    print(data)\n",
    "    #continue\n",
    "\n",
    "lat = js[\"results\"][0][\"geometry\"][\"location\"][\"lat\"]\n",
    "lng = js[\"results\"][0][\"geometry\"][\"location\"][\"lng\"]\n",
    "print('lat', lat, 'lng', lng)\n",
    "location = js['results']\n",
    "print(location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Securing API Requests\n",
    "- Authorization and Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'twurl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-57a5a989d468>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtwurl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mTWITTER_URL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://api.twitter.com/1.1/friends/list.json'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'twurl'"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import twurl\n",
    "import json\n",
    "\n",
    "TWITTER_URL = 'https://api.twitter.com/1.1/friends/list.json'\n",
    "\n",
    "while True:\n",
    "    print('')\n",
    "    acct = input('Enter Twitter Account: ')\n",
    "    if (len(acct) < 1): break\n",
    "    url = twurl.augment(TWITTER_URL,\n",
    "                        {'screen_name': acct, \n",
    "                        'count': 5})\n",
    "    print('Retrieving', url)\n",
    "    connection = urllib.request.urlopen(url)\n",
    "    data = connection.read().decode()\n",
    "    headers = dict(connection.getheaders())\n",
    "    print('Remaining', headers['x-rate-limit-remaining'])\n",
    "    js = json.loads(data)\n",
    "    print(json.dumps(js, indent=4))\n",
    "    \n",
    "    for u in js['users']:\n",
    "        print(u['screen_name'])\n",
    "        s = u['status']['text']\n",
    "        print('  ', s[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: twitter in c:\\users\\sherna\\anaconda3\\lib\\site-packages (1.18.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Idk, skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment - Extracting Data from JSON\n",
    "\n",
    "In this assignment you will write a Python program somewhat similar to http://www.py4e.com/code3/json2.py. The program will prompt for a URL, read the JSON data from that URL using urllib and then parse and extract the comment counts from the JSON data, compute the sum of the numbers in the file and enter the sum below:\n",
    "We provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment.\n",
    "\n",
    "Sample data: http://py4e-data.dr-chuck.net/comments_42.json (Sum=2553)\n",
    "\n",
    "Actual data: http://py4e-data.dr-chuck.net/comments_1088552.json (Sum ends with 88)\n",
    "\n",
    "You do not need to save these files to your folder since your program will read the data directly from the URL. Note: Each student will have a distinct data url for the assignment - so only use your own data url for analysis.\n",
    "\n",
    "Data Format\n",
    "The data consists of a number of names and comment counts in JSON as follows:\n",
    "\n",
    "```\n",
    "{\n",
    "  comments: [\n",
    "    {\n",
    "      name: \"Matthias\"\n",
    "      count: 97\n",
    "    },\n",
    "    {\n",
    "      name: \"Geomer\"\n",
    "      count: 97\n",
    "    }\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "The closest sample code that shows how to parse JSON and extract a list is json2.py. You might also want to look at geoxml.py to see how to prompt for a URL and retrieve data from a URL.\n",
    "\n",
    "Sample Execution\n",
    "\n",
    "```\n",
    "$ python3 solution.py\n",
    "Enter location: http://py4e-data.dr-chuck.net/comments_42.json\n",
    "Retrieving http://py4e-data.dr-chuck.net/comments_42.json\n",
    "Retrieved 2733 characters\n",
    "Count: 50\n",
    "Sum: 2...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter location:  http://py4e-data.dr-chuck.net/comments_1088552.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving http://py4e-data.dr-chuck.net/comments_1088552.json\n",
      "Retrieved 2740 characters\n",
      "Count: 50\n",
      "Sum: 2588\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "#url = 'http://py4e-data.dr-chuck.net/comments_42.json'\n",
    "url = input('Enter location: ')\n",
    "print('Retrieving', url)\n",
    "\n",
    "response = urllib.request.urlopen(url).read()\n",
    "data = response.decode()\n",
    "print('Retrieved', len(data), 'characters')\n",
    "\n",
    "info = json.loads(response)\n",
    "\n",
    "count = 0\n",
    "sum = 0\n",
    "\n",
    "for item in info['comments']:\n",
    "    #print('Name:', item['name'])\n",
    "    #print('Count:', item['count'])\n",
    "    count +=1\n",
    "    sum += int(item['count'])\n",
    "    \n",
    "print('Count:', count)\n",
    "print('Sum:', sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment - Using the GeoJSON API\n",
    "In this assignment you will write a Python program somewhat similar to http://www.py4e.com/code3/geojson.py. The program will prompt for a location, contact a web service and retrieve JSON for the web service and parse that data, and retrieve the first place_id from the JSON. A place ID is a textual identifier that uniquely identifies a place as within Google Maps.\n",
    "API End Points\n",
    "\n",
    "To complete this assignment, you should use this API endpoint that has a static subset of the Google Data:\n",
    "\n",
    "```\n",
    "http://py4e-data.dr-chuck.net/json?\n",
    "```\n",
    "\n",
    "This API uses the same parameter (address) as the Google API. This API also has no rate limit so you can test as often as you like. If you visit the URL with no parameters, you get \"No address...\" response.\n",
    "To call the API, you need to include a key= parameter and provide the address that you are requesting as the address= parameter that is properly URL encoded using the urllib.parse.urlencode() function as shown in http://www.py4e.com/code3/geojson.py\n",
    "\n",
    "Make sure to check that your code is using the API endpoint is as shown above. You will get different results from the geojson and json endpoints so make sure you are using the same end point as this autograder is using.\n",
    "\n",
    "Test Data / Sample Execution\n",
    "\n",
    "You can test to see if your program is working with a location of \"South Federal University\" which will have a place_id of \"ChIJJ2MNmPl_bIcRt8t5x-X5ZhQ\".\n",
    "\n",
    "```\n",
    "$ python3 solution.py\n",
    "Enter location: South Federal University\n",
    "Retrieving http://...\n",
    "Retrieved 2290 characters\n",
    "Place id ChIJJ2MNmPl_bIcRt8t5x-X5ZhQ\n",
    "Turn In\n",
    "```\n",
    "\n",
    "Please run your program to find the place_id for this location:\n",
    "```\n",
    "University of Texas at Austin\n",
    "```\n",
    "\n",
    "Make sure to enter the name and case exactly as above and enter the place_id and your Python code below. Hint: The first seven characters of the place_id are \"ChIJt8- ...\"\n",
    "Make sure to retreive the data from the URL specified above and not the normal Google API. Your program should work with the Google API - but the place_id may not match for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter location:  University of Texas at Austin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving http://py4e-data.dr-chuck.net/json?address=University+of+Texas+at+Austin&key=42\n",
      "Retrieved 1782 characters\n",
      "Place id ChIJt8-EJZu1RIYR3iFKF0_uMYE\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import json\n",
    "import ssl\n",
    "\n",
    "api_key = False\n",
    "# If you have a Google Places API key, enter it here\n",
    "# api_key = 'AIzaSy___IDByT70'\n",
    "# https://developers.google.com/maps/documentation/geocoding/intro\n",
    "\n",
    "if api_key is False:\n",
    "    api_key = 42\n",
    "    serviceurl = 'http://py4e-data.dr-chuck.net/json?'\n",
    "else :\n",
    "    serviceurl = 'https://maps.googleapis.com/maps/api/geocode/json?'\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "while True:\n",
    "    address = input('Enter location: ')\n",
    "    if len(address) < 1: break\n",
    "\n",
    "    parms = dict()\n",
    "    parms['address'] = address\n",
    "    if api_key is not False: parms['key'] = api_key\n",
    "    url = serviceurl + urllib.parse.urlencode(parms)\n",
    "\n",
    "    print('Retrieving', url)\n",
    "    uh = urllib.request.urlopen(url, context=ctx)\n",
    "    data = uh.read().decode()\n",
    "    print('Retrieved', len(data), 'characters')\n",
    "\n",
    "    try:\n",
    "        js = json.loads(data)\n",
    "    except:\n",
    "        js = None\n",
    "\n",
    "    if not js or 'status' not in js or js['status'] != 'OK':\n",
    "        print('==== Failure To Retrieve ====')\n",
    "        print(data)\n",
    "        continue\n",
    "  \n",
    "    placeid = js['results'][0]['place_id']\n",
    "    print('Place id', placeid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
